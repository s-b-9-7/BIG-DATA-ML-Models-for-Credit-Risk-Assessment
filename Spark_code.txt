# -*- coding: utf-8 -*-
"""loan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I7wiwuWdRto3JuETa98WknwPbLTvL2rI

## Overview

This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.

This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

from pyspark.sql.types import *
from pyspark.sql.functions import *

from pyspark.ml import Pipeline
from pyspark.ml.classification import DecisionTreeClassifier, LogisticRegression, GBTClassifier, RandomForestClassifier
from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler
from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator

from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.mllib.evaluation import MulticlassMetrics
from time import time

from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession
from pyspark.storagelevel import StorageLevel

# Commented out IPython magic to ensure Python compatibility.
# %pyspark


IS_SPARK_SUBMIT_CLI = True
if IS_SPARK_SUBMIT_CLI:
    sc = SparkContext.getOrCreate()
    spark = SparkSession(sc)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark


# Limit the log
spark.sparkContext.setLogLevel("WARN")

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

'''
# File location and type
file_location = "/FileStore/tables/loan_sample1000.csv"
file_type = "csv"

# CSV options
infer_schema = "true"
first_row_is_header = "true"
delimiter = ","

# The applied options are for CSV files. For other file types, these will be ignored.
df = spark.read.format(file_type) \
  .option("inferSchema", infer_schema) \
  .option("header", first_row_is_header) \
  .option("sep", delimiter) \
  .load(file_location)

display(df)
'''

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

df = spark.read.csv('/user/sbelurm/loan.csv', inferSchema=True, header=True)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

print('count:', df.count())

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

df = df.select("loan_amnt", "int_rate", "installment", "annual_inc", "fico_range_low", "fico_range_high", "grade", "loan_status")
df.show(5)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

df = df.withColumn("annual_inc", col("annual_inc").cast("double"))
df = df.withColumn("fico_range_low", col("fico_range_low").cast("double"))
df = df.withColumn("fico_range_high", col("fico_range_high").cast("double"))

df.printSchema()

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

grade_mapping = {"A": 1, "B": 2, "C": 3, "D": 4, "E": 5, "F": 6, "G": 7}

df = df.withColumn("grade_numeric", when(df["grade"] == "A", 1)
                                    .when(df["grade"] == "B", 2)
                                    .when(df["grade"] == "C", 3)
                                    .when(df["grade"] == "D", 4)
                                    .when(df["grade"] == "E", 5)
                                    .when(df["grade"] == "F", 6)
                                    .when(df["grade"] == "G", 7)
                                    .otherwise(8))


df = df.withColumn("grade_numeric", df["grade_numeric"].cast(IntegerType()))

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

loan_amnt = df.agg({'loan_amnt': 'mean'}).collect()[0][0]
df = df.fillna(loan_amnt, subset = ["loan_amnt"])

int_rate = df.agg({'int_rate': 'mean'}).collect()[0][0]
df = df.fillna(int_rate, subset = ["int_rate"])

installment = df.agg({'installment': 'mean'}).collect()[0][0]
df = df.fillna(installment, subset = ["installment"])

annual_inc = df.agg({'annual_inc': 'mean'}).collect()[0][0]
df = df.fillna(annual_inc, subset = ["annual_inc"])

fico_range_low = df.agg({'fico_range_low': 'mean'}).collect()[0][0]
df = df.fillna(fico_range_low, subset = ["fico_range_low"])

fico_range_high = df.agg({'fico_range_high': 'mean'}).collect()[0][0]
df = df.fillna(fico_range_high, subset = ["fico_range_high"])

df.show(5)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

df = df.na.replace("nan", None)
df = df.dropna()

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

from pyspark.sql.functions import when

df = df.withColumn("loan_status",
                   when(df["loan_status"].isin("Fully Paid", "Current"), "0")
                   .otherwise("1"))

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

df.show(5)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

df_0 = df.filter(col("loan_status") == 0)
df_1 = df.filter(col("loan_status") == 1)

count_1 = df_1.count()
df_0_sampled = df_0.sample(False, count_1 / df_0.count())

df = df_0_sampled.union(df_1)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

num_zeros = df.filter(df.loan_status == 0).count()

# Print the result
print("Number of zeros in loan_status column:", num_zeros)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

print(df.dtypes)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

print('count:', df.count())

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

splits = df.randomSplit([0.7, 0.3])
train = splits[0]
test = splits[1]
train_rows = train.count()
test_rows = test.count()
print("Training Rows:", train_rows, " Testing Rows:", test_rows)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

#numVect = VectorAssembler(inputCols = ["loan_amnt", "int_rate", "installment", "annual_inc", "fico_range_low", "fico_range_high"], outputCol="numFeatures")
numVect = VectorAssembler(inputCols = ["loan_amnt", "int_rate", "installment", "annual_inc", "fico_range_low", "fico_range_high", "grade_numeric"], outputCol="numFeatures")
minMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol="normFeatures")

#grade_stringIdx = StringIndexer(inputCol = "grade", outputCol = "grade_index", stringOrderType= "alphabetAsc")

#featVect = VectorAssembler(inputCols=["numFeatures", "grade_index"], outputCol="features")
featVect = VectorAssembler(inputCols=["numFeatures"], outputCol="features")

label_stringIdx = StringIndexer(inputCol = "loan_status", outputCol = "label", stringOrderType= "alphabetAsc")

"""#### Logistic Regression"""

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

lr = LogisticRegression(labelCol="label",featuresCol="features")
#pipeline_lr = Pipeline(stages=[numVect, minMax, grade_stringIdx, featVect, label_stringIdx, lr])
pipeline_lr = Pipeline(stages=[numVect, minMax, featVect, label_stringIdx, lr])

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

paramGrid_lr = (ParamGridBuilder() \
             .addGrid(lr.regParam, [0.01, 0.5]) \
             .addGrid(lr.elasticNetParam, [0.0, 0.5]) \
             .addGrid(lr.maxIter, [5, 15]) \
             .build())

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

start = time()

#cv_lr = CrossValidator(estimator=pipeline_lr, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid_lr)
#model_lr = cv_lr.fit(train)
tv_lr = TrainValidationSplit(estimator=pipeline_lr, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid_lr, trainRatio=0.7)
model_lr = tv_lr.fit(train)

end = time()
phrase = 'Logistic Regression testing'
print('{} takes {} seconds'.format(phrase, (end - start))) #round(end - start, 2)))

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

predictions_lr = model_lr.transform(test)

evaluator = BinaryClassificationEvaluator(metricName="areaUnderROC")
auc = evaluator.evaluate(predictions_lr)

print("Logistic Regression")
print("AUC:", auc)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

predictionAndLabels_lr = predictions_lr.select("prediction", "label").rdd
metrics_lr = MulticlassMetrics(predictionAndLabels_lr)
confusion_matrix_lr = metrics_lr.confusionMatrix().toArray()
print("Logistic Regression_Confusion matrix:")
print(confusion_matrix_lr)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

predicted = predictions_lr.select("features", "prediction", "label")
#predicted.show(100, truncate=False)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

tp = float(predicted.filter("prediction == 1.0 AND label == 1").count())
fp = float(predicted.filter("prediction == 1.0 AND label == 0").count())
tn = float(predicted.filter("prediction == 0.0 AND label == 0").count())
fn = float(predicted.filter("prediction == 0.0 AND label == 1").count())
metrics = spark.createDataFrame([
 ("TP", tp),
 ("FP", fp),
 ("TN", tn),
 ("FN", fn),
 ("Accuracy", (tp + tn) / (tp + fp + tn + fn)),
 ("Precision", tp / (tp + fp)),
 ("Recall", tp / (tp + fn))],["metric", "value"])

metrics.show()

"""#### Random Forest"""

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

rf = RandomForestClassifier()
pipeline_rf = Pipeline(stages=[numVect, minMax, featVect, label_stringIdx, rf])

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

paramGrid_rf = (ParamGridBuilder() \
             .addGrid(rf.maxDepth, [5, 15]) \
             .addGrid(rf.maxBins, [10, 20]) \
             .addGrid(rf.numTrees, [25, 50]) \
             .build())

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

start = time()

#cv_rf = CrossValidator(estimator=pipeline_rf, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid_rf)
#model_rf = cv_rf.fit(train)
tv_rf = TrainValidationSplit(estimator=pipeline_rf, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid_rf, trainRatio=0.7)
model_rf = tv_rf.fit(train)

end = time()
phrase = 'Random Forest testing'
print('{} takes {} seconds'.format(phrase, (end - start))) #round(end - start, 2)))

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

predictions_rf = model_rf.transform(test)

evaluator = BinaryClassificationEvaluator(metricName="areaUnderROC")
auc = evaluator.evaluate(predictions_rf)

print("Random Forest")
print("AUC:", auc)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

predictionAndLabels_rf = predictions_rf.select("prediction", "label").rdd
metrics_rf = MulticlassMetrics(predictionAndLabels_rf)
confusion_matrix_rf = metrics_rf.confusionMatrix().toArray()
print("Random Forest_Confusion matrix:")
print(confusion_matrix_rf)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

predicted = predictions_rf.select("features", "prediction", "label")
#predicted.show(100, truncate=False)

tp = float(predicted.filter("prediction == 1.0 AND label == 1").count())
fp = float(predicted.filter("prediction == 1.0 AND label == 0").count())
tn = float(predicted.filter("prediction == 0.0 AND label == 0").count())
fn = float(predicted.filter("prediction == 0.0 AND label == 1").count())
metrics = spark.createDataFrame([
 ("TP", tp),
 ("FP", fp),
 ("TN", tn),
 ("FN", fn),
 ("Accuracy", (tp + tn) / (tp + fp + tn + fn)),
 ("Precision", tp / (tp + fp)),
 ("Recall", tp / (tp + fn))],["metric", "value"])

metrics.show()

"""#### Decision Tree"""

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

dt = DecisionTreeClassifier()
pipeline_dt = Pipeline(stages=[numVect, minMax, featVect, label_stringIdx, dt])

paramGrid_dt = (ParamGridBuilder()
             .addGrid(dt.impurity, ["gini", "entropy"])
             .addGrid(dt.maxDepth, [15, 25])
             .build())

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

start = time()

#cv_dt = CrossValidator(estimator=pipeline_dt, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid_dt)
#model_dt = cv_dt.fit(train)
tv_dt = TrainValidationSplit(estimator=pipeline_dt, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid_dt, trainRatio=0.7)
model_dt = tv_dt.fit(train)

end = time()
phrase = 'Decision Tree testing'
print('{} takes {} seconds'.format(phrase, (end - start))) #round(end - start, 2)))

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

predictions_dt = model_dt.transform(test)

evaluator = BinaryClassificationEvaluator(metricName="areaUnderROC")
auc = evaluator.evaluate(predictions_dt)

print("Decision Tree")
print("AUC:", auc)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

predictionAndLabels_dt = predictions_dt.select("prediction", "label").rdd
metrics_dt = MulticlassMetrics(predictionAndLabels_dt)
confusion_matrix_dt = metrics_dt.confusionMatrix().toArray()
print("Decision Tree_Confusion matrix:")
print(confusion_matrix_dt)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

predicted = predictions_dt.select("features", "prediction", "label")
#predicted.show(100, truncate=False)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

tp = float(predicted.filter("prediction == 1.0 AND label == 1").count())
fp = float(predicted.filter("prediction == 1.0 AND label == 0").count())
tn = float(predicted.filter("prediction == 0.0 AND label == 0").count())
fn = float(predicted.filter("prediction == 0.0 AND label == 1").count())
metrics = spark.createDataFrame([
 ("TP", tp),
 ("FP", fp),
 ("TN", tn),
 ("FN", fn),
 ("Accuracy", (tp + tn) / (tp + fp + tn + fn)),
 ("Precision", tp / (tp + fp)),
 ("Recall", tp / (tp + fn))],["metric", "value"])

metrics.show()

"""#### GBT"""

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

gbt = GBTClassifier()
pipeline_gbt = Pipeline(stages=[numVect, minMax, featVect, label_stringIdx, gbt])

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

paramGrid_gbt = (ParamGridBuilder()
             .addGrid(gbt.maxDepth, [5, 15])
             .addGrid(gbt.maxBins, [10, 20])
             .addGrid(gbt.maxIter, [15])
             .build())

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

start = time()

#cv_gbt = CrossValidator(estimator=pipeline_gbt, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid_gbt)
#model_gbt = cv_gbt.fit(train)
tv_gbt = TrainValidationSplit(estimator=pipeline_gbt, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid_gbt, trainRatio=0.7)
model_gbt = tv_gbt.fit(train)

end = time()
phrase = 'Gradient-Boosted Tree Classifier testing'
print('{} takes {} seconds'.format(phrase, (end - start))) #round(end - start, 2)))

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

predictions_gbt = model_gbt.transform(test)

evaluator = BinaryClassificationEvaluator(metricName="areaUnderROC")
auc = evaluator.evaluate(predictions_gbt)

print("GBT")
print("AUC:", auc)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

predictionAndLabels_gbt = predictions_gbt.select("prediction", "label").rdd
metrics_gbt = MulticlassMetrics(predictionAndLabels_gbt)
confusion_matrix_gbt = metrics_gbt.confusionMatrix().toArray()
print("GBT_Confusion matrix:")
print(confusion_matrix_gbt)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

predicted = predictions_gbt.select("features", "prediction", "label")
#predicted.show(100, truncate=False)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

tp = float(predicted.filter("prediction == 1.0 AND label == 1").count())
fp = float(predicted.filter("prediction == 1.0 AND label == 0").count())
tn = float(predicted.filter("prediction == 0.0 AND label == 0").count())
fn = float(predicted.filter("prediction == 0.0 AND label == 1").count())
metrics = spark.createDataFrame([
 ("TP", tp),
 ("FP", fp),
 ("TN", tn),
 ("FN", fn),
 ("Accuracy", (tp + tn) / (tp + fp + tn + fn)),
 ("Precision", tp / (tp + fp)),
 ("Recall", tp / (tp + fn))],["metric", "value"])

metrics.show()

